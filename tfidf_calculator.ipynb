{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import uuid\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import traceback\n",
    "import csv\n",
    "\n",
    "import tfidf_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in an entire story based on document id\n",
    "def get_document_text(document_id):\n",
    "    try:\n",
    "        text=\"\"\n",
    "        try:\n",
    "            f=open(f\"../narrativeqa/tmp/{document_id}.content\", \"r\", encoding=\"utf-8\")\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "        except:\n",
    "            f=open(f\"../narrativeqa/tmp/{document_id}.content\", \"r\", encoding=\"ISO-8859-1\")\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting document {document_id}\")\n",
    "        print(f\"Exception: {e}\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def split_document(text, num_characters):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into passages to serve as documents in tfidf\n",
    "def split_document_and_tfidf_vectorize(text, num_characters=1500):\n",
    "    passages = [text[i:i+num_characters] for i in range(0, len(text), num_characters)]\n",
    "\n",
    "    #passages = text.split('\\n\\n')\n",
    "\n",
    "    #passages = list(filter(None, passages))\n",
    "    #print(len(passages))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=set(stopwords.words(\"english\")))\n",
    "    try:\n",
    "        tfidf = vectorizer.fit_transform(passages)\n",
    "        return passages, tfidf, vectorizer\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        print(passages)\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAPair:\n",
    "    passages = []\n",
    "    \n",
    "    def __init__(self, document_id, question, answer1, answer2):\n",
    "        self.document_id = document_id\n",
    "        self.question = question\n",
    "        self.answer1 = answer1\n",
    "        self.answer2 = answer2\n",
    "        self.id = uuid.uuid4()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all question answer pairs for the available documents\n",
    "#document_id, set, question, answer1, answer2, question_tokenized, answer1_tokenized, answer2_tokenized.\n",
    "def get_question_answer_pairs():\n",
    "    document_questions = {}\n",
    "    with open('../narrativeqa/qaps.csv', newline='') as csvfile:\n",
    "        rows = csv.DictReader(csvfile, delimiter=',')\n",
    "        for qpair in rows:\n",
    "            document_id = qpair['document_id']\n",
    "\n",
    "            if document_id not in document_questions.keys():\n",
    "                document_questions[document_id] = []\n",
    "\n",
    "            document_questions[document_id].append(QAPair(document_id, qpair['question_tokenized'], qpair['answer1'], qpair['answer2']))\n",
    "\n",
    "    return document_questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_questions = get_question_answer_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top n passage indices in regards to a query within a document\n",
    "# Based on cosine simliarity\n",
    "def get_related_passage_indices(question, vectorizer, tfidf, num_passages_to_return=5):\n",
    "    q_vec = vectorizer.transform([question])\n",
    "    cosine_similarities = linear_kernel(q_vec, tfidf).flatten()\n",
    "\n",
    "    related_docs_indices_a = cosine_similarities.argsort()[:-num_passages_to_return:-1]\n",
    "    related_docs_indices = []\n",
    "    for index in related_docs_indices_a:\n",
    "        if abs(cosine_similarities[index]) > 0:\n",
    "            related_docs_indices.append(index)\n",
    "            \n",
    "    #if len(related_docs_indices) == 0:\n",
    "    #    print(f'empty question: {question}')\n",
    "    #    print(f'empty qvec: {q_vec}')\n",
    "    \n",
    "    return related_docs_indices\n",
    "\n",
    "# Get the top n passages in regards to a query\n",
    "def get_related_passages(passages, related_docs_indices):\n",
    "    related_passages = []\n",
    "    for i in related_docs_indices:\n",
    "        related_passages.append(passages[i])\n",
    "    \n",
    "    return related_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "\n",
    "#q1 = [document_questions[\"0029bdbe75423337b551e42bb31f9a102785376f\"][1].question]\n",
    "#answer1 = document_questions[\"0029bdbe75423337b551e42bb31f9a102785376f\"][1].answer1\n",
    "#answer2 = document_questions[\"0029bdbe75423337b551e42bb31f9a102785376f\"][1].answer2\n",
    "#q_vec = vectorizer.transform(q1)\n",
    "#cosine_similarities = linear_kernel(q_vec, tfidf).flatten()\n",
    "\n",
    "#num_passages_to_return = 5\n",
    "#related_docs_indices_a = cosine_similarities.argsort()[:-num_passages_to_return:-1]\n",
    "#related_docs_indices = []\n",
    "#or index in related_docs_indices_a:\n",
    "#    if cosine_similarities[index] > 0:\n",
    "#        related_docs_indices.append(index)\n",
    "        \n",
    "#print(f'Q: {q1}\\n')\n",
    "#for i in related_docs_indices:\n",
    "#    print(f'index: {i}')\n",
    "#    print(passages[i])\n",
    "#    print()\n",
    "#print('\\nAnswers')\n",
    "#print(answer1)\n",
    "#print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   {\n",
    "# .   title:\n",
    "# .   document_id:\n",
    "# .   paragraphs:[\n",
    "#{                    \"qas\": [\n",
    "#                        {\n",
    "#                            \"question\": \"In what country is Normandy located?\",\n",
    "#                            \"id\": \"56ddde6b9a695914005b9628\",\n",
    "#                            \"answers\": [\n",
    "#                                {\n",
    "#                                    \"text\": \"France\",\n",
    "#                                    \"answer_start\": 159\n",
    "#                                },\n",
    "#                                {\n",
    "#                                    \"text\": \"France\",\n",
    "#                                    \"answer_start\": 159\n",
    "#                                },\n",
    "#\n",
    "#                            ],\n",
    "#                            \"is_impossible\": false\n",
    "#                        },}\n",
    "#                       #context:\n",
    "\n",
    "def convert_question_pair_to_squad_format(qa_pair):\n",
    "    data = {\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"question\": qa_pair.question,\n",
    "                \"id\": str(qa_pair.id),\n",
    "                \"answers\": [\n",
    "                    {\n",
    "                        \"text\": qa_pair.answer1\n",
    "                    },\n",
    "                    {\n",
    "                        \"text\": qa_pair.answer2\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"context\": qa_pair.passages[0]\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_start(text, start_search):\n",
    "    doc_start = text.find(start_search)\n",
    "    if doc_start == -1:\n",
    "        start_search = \"*** START \"\n",
    "        doc_start = text.find(start_search, 0)\n",
    "        if doc_start == -1:\n",
    "            start_search = \"***START \"\n",
    "            doc_start = text.find(start_search, 0)\n",
    "            if doc_start == -1:\n",
    "                start_search = \"<pre>\"\n",
    "                doc_start = text.find(start_search)\n",
    "    return doc_start, start_search\n",
    "\n",
    "def get_doc_end(text, end_search):\n",
    "    doc_end = text.rfind(end_search)\n",
    "    if doc_end == -1:\n",
    "        end_search = \"*** END\"\n",
    "        doc_end = text.rfind(end_search)\n",
    "        if doc_end == -1:\n",
    "            end_search = \"***END\"\n",
    "            doc_end = text.rfind(end_search)\n",
    "            if doc_end == -1:\n",
    "                end_search = \"</pre>\"\n",
    "                doc_end = text.rfind(end_search)\n",
    "    return doc_end, end_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                   print('-------11111--------------')\n",
    "#                        print('\\ntext length 0')\n",
    "#                        print(f'docid: {document_id}')\n",
    "#                        print(f'labeled start: {x_split[8]}')\n",
    "#                        print(f'labeled end: {x_split[9]}')\n",
    "#                        print(f'dstart: {doc_start}')\n",
    "#                        print(f'dend: {doc_end}')\n",
    "#                        print(f'start search: {start_search}')\n",
    "#                        print(f'end search: {end_search}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through available documents, retrieve the top passages for each question/answer pair\n",
    "# Write the returned passages to document_qa_passages directory for later use\n",
    "# Pairs written as directionary of form {q: [passage, passage, etc]}\n",
    "\n",
    "#document_id,set,kind,story_url,story_file_size,wiki_url,wiki_title,story_word_count,story_start,story_end\n",
    "\n",
    "# {\n",
    "# version:\n",
    "# data: [\n",
    "\n",
    "#}       \n",
    "#]\n",
    "#}\n",
    "#]\n",
    "#}\n",
    "def get_and_write_qa_passages_as_squad(document_questions, max_stories=-1):\n",
    "    with open('../narrativeqa/documents.csv') as f1:\n",
    "        rows = csv.DictReader(f1, delimiter=',')\n",
    "        i = 0\n",
    "        s = 0\n",
    "        q = 0\n",
    "        document_id=\"\"\n",
    "        data = {}\n",
    "        data['version']=\"1.0\"\n",
    "        data['data'] = []\n",
    "        for doc in rows:\n",
    "            try:\n",
    "                i = i + 1\n",
    "                if i == 1:\n",
    "                    continue\n",
    "\n",
    "                book_data = {}\n",
    "                document_id = doc['document_id']\n",
    "\n",
    "                book_data['title'] = doc['wiki_title']\n",
    "                book_data['document_id'] = document_id\n",
    "                book_data['paragraphs'] = []\n",
    "\n",
    "\n",
    "                text = get_document_text(document_id)\n",
    "                text = ' '.join(text.split())\n",
    "\n",
    "                doc_start, start_search = get_doc_start(text, doc['story_start'])\n",
    "                doc_end, end_search = get_doc_end(text, doc['story_end'])\n",
    "\n",
    "                if doc_start != -1:\n",
    "                    text = text[int(doc_start):int(doc_end)]\n",
    "\n",
    "                passages, tfidf, vectorizer = split_document_and_tfidf_vectorize(text)\n",
    "                passages_to_write = {}\n",
    "\n",
    "                for qa_pair in document_questions[document_id]:\n",
    "                    q = q + 1\n",
    "                    related_indices = get_related_passage_indices(qa_pair.question, vectorizer, tfidf, num_passages_to_return=5)\n",
    "                    related_passages = get_related_passages(passages, related_indices)\n",
    "                    qa_pair.passages = related_passages\n",
    "\n",
    "                    if len(qa_pair.passages) == 0:\n",
    "                        #print(f'skipped pair: {document_id}')\n",
    "                        s = s + 1\n",
    "                        continue\n",
    "\n",
    "                    book_data['paragraphs'].append(convert_question_pair_to_squad_format(qa_pair))\n",
    "                    passages_to_write[qa_pair.question] = related_passages\n",
    "\n",
    "                json_question_pairs = json.dumps(passages_to_write)\n",
    "                fq = open(f\"./document_qa_passages/{document_id}.q_passages\", \"w\")\n",
    "                fq.write(json_question_pairs)\n",
    "                fq.close()\n",
    "\n",
    "                data['data'].append(book_data)\n",
    "\n",
    "                if max_stories != -1 and i > max_stories:\n",
    "                    break\n",
    "\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "                print(f\"Error processing qa pairs and passages for document {document_id}. Story no {i}\")\n",
    "                break\n",
    "\n",
    "        squad_json_format_qa = json.dumps(data)\n",
    "        fq = open(f\"./squad_document_qa_passages/train.data\", \"w\")\n",
    "        fq.write(squad_json_format_qa)\n",
    "        fq.close()\n",
    "        \n",
    "        print(f'total question pairs: {q}')\n",
    "        print(f'skipped pairs: {s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total question pairs: 46736\n",
      "skipped pairs: 845\n"
     ]
    }
   ],
   "source": [
    "get_and_write_qa_passages_as_squad(document_questions, max_stories=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#clear html\n",
    "#try different combinations of characters\n",
    "#break up by paragraphs?\n",
    "    #heuristic to aim for size?\n",
    "\n",
    "#probability of passage  being correct?\n",
    "\n",
    "#catalog sparknotes urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
