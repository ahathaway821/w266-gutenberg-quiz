{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import uuid\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import traceback\n",
    "\n",
    "import tfidf_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in an entire story based on document id\n",
    "def get_document_text(document_id):\n",
    "    try:\n",
    "        text=\"\"\n",
    "        try:\n",
    "            f=open(f\"../narrativeqa/tmp/{document_id}.content\", \"r\", encoding=\"utf-8\")\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "        except:\n",
    "            f=open(f\"../narrativeqa/tmp/{document_id}.content\", \"r\", encoding=\"ISO-8859-1\")\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting document {document_id}\")\n",
    "        print(f\"Exception: {e}\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into passages to serve as documents in tfidf\n",
    "def split_document_and_tfidf_vectorize(text, num_characters=500):\n",
    "    passages = [text[i:i+num_characters] for i in range(0, len(text), num_characters)]\n",
    "\n",
    "    #passages = text.split('\\n\\n')\n",
    "\n",
    "    #passages = list(filter(None, passages))\n",
    "    #print(len(passages))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=set(stopwords.words(\"english\")))\n",
    "    tfidf = vectorizer.fit_transform(passages)\n",
    "    \n",
    "    return passages, tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAPair:\n",
    "    passages = []\n",
    "    \n",
    "    def __init__(self, document_id, question, answer1, answer2):\n",
    "        self.document_id = document_id\n",
    "        self.question = question\n",
    "        self.answer1 = answer1\n",
    "        self.answer2 = answer2\n",
    "        self.id = uuid.uuid4()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all question answer pairs for the available documents\n",
    "#document_id, set, question, answer1, answer2, question_tokenized, answer1_tokenized, answer2_tokenized.\n",
    "def get_question_answer_pairs():\n",
    "    \n",
    "    document_questions = {}\n",
    "    \n",
    "    f=open(f\"../narrativeqa/qaps.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "    f1 = f.readlines()\n",
    "    for x in f1:\n",
    "        x_split = x.split(',')\n",
    "        document_id = str(x_split[0])\n",
    "        if document_id not in document_questions.keys():\n",
    "            document_questions[document_id] = []\n",
    "        \n",
    "        document_questions[document_id].append(QAPair(x_split[0], x_split[2], x_split[3], x_split[4]))\n",
    "    \n",
    "    return document_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_questions = get_question_answer_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top n passage indices in regards to a query within a document\n",
    "# Based on cosine simliarity\n",
    "def get_related_passage_indices(question, vectorizer, tfidf, num_passages_to_return=5):\n",
    "    q_vec = vectorizer.transform([question])\n",
    "    cosine_similarities = linear_kernel(q_vec, tfidf).flatten()\n",
    "\n",
    "    related_docs_indices_a = cosine_similarities.argsort()[:-num_passages_to_return:-1]\n",
    "    related_docs_indices = []\n",
    "    for index in related_docs_indices_a:\n",
    "        if cosine_similarities[index] > 0:\n",
    "            related_docs_indices.append(index)\n",
    "            \n",
    "    return related_docs_indices\n",
    "\n",
    "# Get the top n passages in regards to a query\n",
    "def get_related_passages(passages, related_docs_indices):\n",
    "    related_passages = []\n",
    "    for i in related_docs_indices:\n",
    "        related_passages.append(passages[i])\n",
    "    \n",
    "    return related_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ['Who does Arabella Mason wed?']\n",
      "\n",
      "index: 6\n",
      "he\n",
      "captain, liked attention, and liked sailors; this was Miss Arabella\n",
      "Mason, a very pretty young woman of eighteen years of age, who\n",
      "constantly looked in the glass merely to ascertain if she had ever seen\n",
      "a face which she preferred to her own, and who never read any novel\n",
      "without discovering that there was a remarkable likeness between the\n",
      "heroine and her pretty self.\n",
      "\n",
      "Miss Arabella Mason was the eldest daughter of the steward of the old\n",
      "Lord de Versely, brother to the Honourable Miss Delmar, a\n",
      "\n",
      "index: 17\n",
      "again came over to Madeline Hall, accompanied\n",
      "as usual, by Ben, and the second day after their arrival it was made\n",
      "known to all whom it might concern, that Miss Arabella Mason had\n",
      "actually contracted a secret marriage with the handsome Benjamin Keene.\n",
      "\n",
      "Of course, the last person made acquainted with this interesting\n",
      "intelligence was the Honourable Miss Delmar, and her nephew took upon\n",
      "himself to make the communication.  At first the honourable spinster\n",
      "bridled up with indignation, wondered at th\n",
      "\n",
      "index: 365\n",
      "er severe with a brother mason.\"\n",
      "\n",
      "\"But how did he know you were a mason?\"\n",
      "\n",
      "\"I made the sign to him the very first time that he began to scold me,\n",
      "and he left off almost immediately; that is, when I made the second\n",
      "sign; he did not when I made the first.\"\n",
      "\n",
      "\"I should like to know these signs.  Won't you tell them to me?\"\n",
      "\n",
      "\"Tell them to you! oh no, that won't do,\" replied I.  \"I don't know you.\n",
      "Here we are on board--in bow,--rowed of all, men.  Now, Mr Green, I'll\n",
      "show you the way up.\"\n",
      "\n",
      "Mr Green wa\n",
      "\n",
      "index: 14\n",
      "olly uneducated--for he was too stupid to learn--his\n",
      "faculties were just sufficient to enable him, by constant drilling, to\n",
      "be perfect in the manual exercise, and mechanically to perform his\n",
      "duties as a valet.\n",
      "\n",
      "Ben always accompanied his master to the hall, where the former was at\n",
      "one and the same time the admiration and laughter of all the servants.\n",
      "It hardly need be observed, that the clever and sprightly Miss Arabella\n",
      "Mason considered Ben as one much beneath her, that is, she said so on\n",
      "his f\n",
      "\n",
      "\n",
      "Answers\n",
      "\"Ben Keene\n",
      " Delmar's valet\"\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "\n",
    "q1 = [document_questions[\"0029bdbe75423337b551e42bb31f9a102785376f\"][1].question]\n",
    "answer1 = document_questions[\"0029bdbe75423337b551e42bb31f9a102785376f\"][1].answer1\n",
    "answer2 = document_questions[\"0029bdbe75423337b551e42bb31f9a102785376f\"][1].answer2\n",
    "q_vec = vectorizer.transform(q1)\n",
    "cosine_similarities = linear_kernel(q_vec, tfidf).flatten()\n",
    "\n",
    "num_passages_to_return = 5\n",
    "related_docs_indices_a = cosine_similarities.argsort()[:-num_passages_to_return:-1]\n",
    "related_docs_indices = []\n",
    "for index in related_docs_indices_a:\n",
    "    if cosine_similarities[index] > 0:\n",
    "        related_docs_indices.append(index)\n",
    "        \n",
    "print(f'Q: {q1}\\n')\n",
    "for i in related_docs_indices:\n",
    "    print(f'index: {i}')\n",
    "    print(passages[i])\n",
    "    print()\n",
    "print('\\nAnswers')\n",
    "print(answer1)\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through available documents, retrieve the top passages for each question/answer pair\n",
    "# Write the returned passages to document_qa_passages directory for later use\n",
    "# Pairs written as directionary of form {q: [passage, passage, etc]}\n",
    "\n",
    "#document_id,set,kind,story_url,story_file_size,wiki_url,wiki_title,story_word_count,story_start,story_end\n",
    "def get_and_write_qa_passages(document_questions, max_stories=2):\n",
    "    f=open(\"../narrativeqa/documents.csv\", \"r\")\n",
    "    f1 = f.readlines()\n",
    "    i = 0\n",
    "    max_stories = 2\n",
    "    document_id=\"\"\n",
    "    for x in f1:\n",
    "        try:\n",
    "            i = i + 1\n",
    "            if i == 1:\n",
    "                continue\n",
    "\n",
    "            x_split = x.split(',')\n",
    "            document_id = x_split[0]\n",
    "\n",
    "            text = get_document_text(document_id)\n",
    "\n",
    "            doc_start = text.find(x_split[8])\n",
    "            doc_end = text.rfind(x_split[9])\n",
    "\n",
    "            text = text[int(doc_start):int(doc_end)]\n",
    "\n",
    "            passages, tfidf, vectorizer = split_document_and_tfidf_vectorize(text)\n",
    "            passages_to_write = {}\n",
    "            \n",
    "            for qa_pair in document_questions[document_id]:\n",
    "                related_indices = get_related_passage_indices(qa_pair.question, vectorizer, tfidf, num_passages_to_return=5)\n",
    "                related_passages = get_related_passages(passages, related_indices)\n",
    "                qa_pair.passages = related_passages\n",
    "                \n",
    "                passages_to_write[qa_pair.question] = related_passages\n",
    "\n",
    "            json_question_pairs = json.dumps(passages_to_write)\n",
    "            fq = open(f\"./document_qa_passages/{document_id}.q_passages\", \"w\")\n",
    "            fq.write(json_question_pairs)\n",
    "            fq.close()\n",
    "\n",
    "            if i > max_stories and max_stories != -1:\n",
    "                break\n",
    "                \n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            print(f\"Error processing qa pairs and passages for document {document_id}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_and_write_qa_passages(document_questions, max_stories=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
