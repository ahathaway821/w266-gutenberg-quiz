{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import nltk\nimport nltk.data\nfrom nltk.corpus import stopwords\nimport uuid\nimport json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nimport traceback\nimport csv\n\nimport tfidf_functions"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"from html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.fed = []\n    def handle_data(self, d):\n        self.fed.append(d)\n    def get_data(self):\n        return ''.join(self.fed)\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"# Read in an entire story based on document id\ndef get_document_text(document_id):\n    try:\n        text=\"\"\n        try:\n            f=open(f\"../narrativeqa/tmp/{document_id}.content\", \"r\", encoding=\"utf-16\")\n            text = f.read()\n            f.close()\n        except:\n            f=open(f\"../narrativeqa/tmp/{document_id}.content\", \"r\", encoding=\"ISO-8859-1\")\n            text = f.read()\n            f.close()\n            \n        #text = strip_tags(text)\n        return text\n    except Exception as e:\n        print(f\"Error getting document {document_id}\")\n        print(f\"Exception: {e}\")        \n"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"# Split text into passages to serve as documents in tfidf\ndef split_document_and_tfidf_vectorize_paragraphs(text, document_id, num_characters=1500):\n    #passages = [text[i:i+num_characters] for i in range(0, len(text), num_characters)]\n\n    paragraphs = text.split('\\n\\n')\n    \n    if len(paragraphs) < 10:\n        print(text)\n        print(paragraphs)\n        print('heeeelp')\n        print(f'doc id: {document_id}')\n        return\n    \n    passages = []\n    passage_text = \"\"\n    \n    for p in paragraphs:\n        passage_text = passage_text + p.replace(\" \", \"\")\n        if len(passage_text) > num_characters - 100:\n            no_tags = strip_tags(passage_text)\n            passages.append(no_tags)\n            passage_text = \"\"\n\n    vectorizer = TfidfVectorizer(stop_words=set(stopwords.words(\"english\")))\n    try:\n        tfidf = vectorizer.fit_transform(passages)\n        return passages, tfidf, vectorizer\n    except Exception:\n        traceback.print_exc()\n        print(passages)\n        \n\n    \n"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"# Split text into passages to serve as documents in tfidf\ndef split_document_and_tfidf_vectorize_characters(text, num_characters=1500):\n    passages = [text[i:i+num_characters] for i in range(0, len(text), num_characters)]\n\n    #passages = text.split('\\n\\n')\n\n    #passages = list(filter(None, passages))\n    #print(len(passages))\n\n    vectorizer = TfidfVectorizer(stop_words=set(stopwords.words(\"english\")))\n    try:\n        tfidf = vectorizer.fit_transform(passages)\n        return passages, tfidf, vectorizer\n    except Exception:\n        traceback.print_exc()\n        print(passages)\n        \n\n    \n"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"# Split text into passages to serve as documents in tfidf\ndef split_document_and_tfidf_vectorize_sentences(text, document_id, num_characters=1500):\n\n    text = strip_tags(text)\n    sentences = sent_detector.tokenize(text.strip())\n    \n    if len(sentences) < 10:\n        print(text)\n        print(sentences)\n        print('heeeelp')\n        print(f'doc id: {document_id}')\n        return\n    \n    passages = []\n    sentence_text = \"\"\n    \n    for s in sentences:\n        sentence_text = sentence_text + \" \" + \" \".join(s.split())\n        if len(sentence_text) > num_characters - 100:\n            passages.append(sentence_text)\n            sentence_text = \"\"\n\n    vectorizer = TfidfVectorizer(stop_words=set(stopwords.words(\"english\")))\n    try:\n        tfidf = vectorizer.fit_transform(passages)\n        return passages, tfidf, vectorizer\n    except Exception:\n        traceback.print_exc()\n        print(passages)\n"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"def split_document_and_tfidf_vectorize(text, document_id):\n    #return split_document_and_tfidf_vectorize_paragraphs(text, document_id)\n    return split_document_and_tfidf_vectorize_sentences(text, document_id)\n"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"class QAPair:\n    passages = []\n    \n    def __init__(self, document_id, question, answer1, answer2, set_split):\n        self.document_id = document_id\n        self.question = question\n        self.answer1 = answer1\n        self.answer2 = answer2\n        self.id = uuid.uuid4()\n        self.set_split = set_split\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"# Load all question answer pairs for the available documents\n#document_id, set, question, answer1, answer2, question_tokenized, answer1_tokenized, answer2_tokenized.\ndef get_question_answer_pairs():\n    document_questions = {}\n    with open('../narrativeqa/qaps.csv', newline='') as csvfile:\n        rows = csv.DictReader(csvfile, delimiter=',')\n        for qpair in rows:\n            document_id = qpair['document_id']\n\n            if document_id not in document_questions.keys():\n                document_questions[document_id] = []\n\n            document_questions[document_id].append(QAPair(document_id, qpair['question_tokenized'], qpair['answer1'], qpair['answer2'], qpair['set']))\n\n    return document_questions\n\n"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"document_questions = get_question_answer_pairs()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"# Get the top n passage indices in regards to a query within a document\n# Based on cosine simliarity\ndef get_related_passage_indices(question, vectorizer, tfidf, num_passages_to_return=5):\n    q_vec = vectorizer.transform([question])\n    cosine_similarities = linear_kernel(q_vec, tfidf).flatten()\n\n    related_docs_indices_a = cosine_similarities.argsort()[:-num_passages_to_return:-1]\n    related_docs_indices = []\n    for index in related_docs_indices_a:\n        if abs(cosine_similarities[index]) > 0:\n            related_docs_indices.append(index)\n            \n    #if len(related_docs_indices) == 0:\n    #    print(f'empty question: {question}')\n    #    print(f'empty qvec: {q_vec}')\n    \n    return related_docs_indices\n\n# Get the top n passages in regards to a query\ndef get_related_passages(passages, related_docs_indices):\n    related_passages = []\n    for i in related_docs_indices:\n        related_passages.append(passages[i])\n    \n    return related_passages"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"#TESTING\n\n#q1 = [document_questions[\"0029bdbe75423337b551e42bb31f9a102785376f\"][1].question]\n#answer1 = document_questions[\"0029bdbe75423337b551e42bb31f9a102785376f\"][1].answer1\n#answer2 = document_questions[\"0029bdbe75423337b551e42bb31f9a102785376f\"][1].answer2\n#q_vec = vectorizer.transform(q1)\n#cosine_similarities = linear_kernel(q_vec, tfidf).flatten()\n\n#num_passages_to_return = 5\n#related_docs_indices_a = cosine_similarities.argsort()[:-num_passages_to_return:-1]\n#related_docs_indices = []\n#or index in related_docs_indices_a:\n#    if cosine_similarities[index] > 0:\n#        related_docs_indices.append(index)\n        \n#print(f'Q: {q1}\\n')\n#for i in related_docs_indices:\n#    print(f'index: {i}')\n#    print(passages[i])\n#    print()\n#print('\\nAnswers')\n#print(answer1)\n#print(answer2)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"#   {\n# .   title:\n# .   document_id:\n# .   paragraphs:[\n#{                    \"qas\": [\n#                        {\n#                            \"question\": \"In what country is Normandy located?\",\n#                            \"id\": \"56ddde6b9a695914005b9628\",\n#                            \"answers\": [\n#                                {\n#                                    \"text\": \"France\",\n#                                    \"answer_start\": 159\n#                                },\n#                                {\n#                                    \"text\": \"France\",\n#                                    \"answer_start\": 159\n#                                },\n#\n#                            ],\n#                            \"is_impossible\": false\n#                        },}\n#                       #context:\n\ndef convert_question_pair_to_squad_format(qa_pair):\n    data = {\n        \"qas\": [\n            {\n                \"question\": qa_pair.question,\n                \"id\": str(qa_pair.id),\n                \"answers\": [\n                    {\n                        \"text\": qa_pair.answer1\n                    },\n                    {\n                        \"text\": qa_pair.answer2\n                    }\n                ]\n            }\n        ],\n        \"context\": qa_pair.passages[0]\n    }\n    \n    return data"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"def get_doc_start(text, start_search):\n    doc_start = text.find(start_search)\n    if doc_start == -1:\n        start_search = \"*** START \"\n        doc_start = text.find(start_search, 0)\n        if doc_start == -1:\n            start_search = \"***START \"\n            doc_start = text.find(start_search, 0)\n            if doc_start == -1:\n                start_search = \"<pre>\"\n                doc_start = text.find(start_search)\n    return doc_start, start_search\n\ndef get_doc_end(text, end_search):\n    doc_end = text.rfind(end_search)\n    if doc_end == -1:\n        end_search = \"*** END\"\n        doc_end = text.rfind(end_search)\n        if doc_end == -1:\n            end_search = \"***END\"\n            doc_end = text.rfind(end_search)\n            if doc_end == -1:\n                end_search = \"</pre>\"\n                doc_end = text.rfind(end_search)\n    return doc_end, end_search"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"#                   print('-------11111--------------')\n#                        print('\\ntext length 0')\n#                        print(f'docid: {document_id}')\n#                        print(f'labeled start: {x_split[8]}')\n#                        print(f'labeled end: {x_split[9]}')\n#                        print(f'dstart: {doc_start}')\n#                        print(f'dend: {doc_end}')\n#                        print(f'start search: {start_search}')\n#                        print(f'end search: {end_search}')"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"def document_is_on_skip_list(document_id):\n    skip_list = ['09355a61a4d84807f9533f31d3263809cc486b6b',\n                 \"492f2d56eba93816e7d0958e2ba62d36d93bc97e\", \n                 \"bd14fef15878fdac1e9c2d2dbe52df0951f38aad\",\n                \"1aae28477e771b3af008ec59ce29086a1bc66776\",\n                \"3747036f950fe8f79cdaa0eb713104b9eb8af6c5\",\n                \"5283fa0a6ea69f4b4224d12018bbf985a2b80543\"]\n    if document_id in skip_list:\n        return True\n    return False"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":"def set_data_split(data, book_data, data_split, train_data, valid_data, test_data):\n    data['data'].append(book_data)\n    \n    if data_split == \"train\":\n        train_data['data'].append(book_data)\n    elif data_split == \"valid\":\n        valid_data['data'].append(book_data)\n    elif data_split == \"test\":\n        test_data['data'].append(book_data)"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":"def save_train_valid_test_data(data, train_data, valid_data, test_data):\n    squad_json_format_qa = json.dumps(data)\n    fq = open(f\"./squad_document_qa_passages/all.data\", \"w\")\n    fq.write(squad_json_format_qa)\n    fq.close()\n    \n    squad_json_format_qa = json.dumps(train_data)\n    fq = open(f\"./squad_document_qa_passages/train.data\", \"w\")\n    fq.write(squad_json_format_qa)\n    fq.close()\n    \n    squad_json_format_qa = json.dumps(valid_data)\n    fq = open(f\"./squad_document_qa_passages/valid.data\", \"w\")\n    fq.write(squad_json_format_qa)\n    fq.close()\n    \n    squad_json_format_qa = json.dumps(test_data)\n    fq = open(f\"./squad_document_qa_passages/test.data\", \"w\")\n    fq.write(squad_json_format_qa)\n    fq.close()"},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":"# Loop through available documents, retrieve the top passages for each question/answer pair\n# Write the returned passages to document_qa_passages directory for later use\n# Pairs written as directionary of form {q: [passage, passage, etc]}\n\n#document_id,set,kind,story_url,story_file_size,wiki_url,wiki_title,story_word_count,story_start,story_end\n\n# {\n# version:\n# data: [\n\n#}       \n#]\n#}\n#]\n#}\ndef get_and_write_qa_passages_as_squad(document_questions, max_stories=-1):\n    with open('../narrativeqa/documents.csv') as f1:\n        rows = csv.DictReader(f1, delimiter=',')\n        i = 0\n        s = 0\n        q = 0\n        document_id=\"\"\n        \n        data = {}\n        data['version']=\"1.0\"\n        data['data'] = []\n        \n        train_data = {}\n        train_data['version']=\"1.0\"\n        train_data['data'] = []\n        \n        valid_data = {}\n        valid_data['version']=\"1.0\"\n        valid_data['data'] = []\n        \n        test_data = {}\n        test_data['version']=\"1.0\"\n        test_data['data'] = []\n        \n        for doc in rows:\n            try:\n                i = i + 1\n                if i == 1:\n                    continue\n\n                book_data = {}\n                document_id = doc['document_id']\n                \n                if document_is_on_skip_list(document_id):\n                    continue\n\n                book_data['title'] = doc['wiki_title']\n                book_data['document_id'] = document_id\n                book_data['paragraphs'] = []\n\n\n                text = get_document_text(document_id)\n                text = text.replace('</b><b>', '\\n\\n')\n                #text = ' '.join(text.split())\n\n                doc_start, start_search = get_doc_start(text, doc['story_start'])\n                doc_end, end_search = get_doc_end(text, doc['story_end'])\n\n                if doc_start != -1:\n                    text = text[int(doc_start):int(doc_end)]\n\n                passages, tfidf, vectorizer = split_document_and_tfidf_vectorize(text, document_id)\n                passages_to_write = {}\n                data_split = \"\"\n                \n                for qa_pair in document_questions[document_id]:\n                    q = q + 1\n                    related_indices = get_related_passage_indices(qa_pair.question, vectorizer, tfidf, num_passages_to_return=5)\n                    related_passages = get_related_passages(passages, related_indices)\n                    qa_pair.passages = related_passages\n\n                    if len(qa_pair.passages) == 0:\n                        #print(f'skipped pair: {document_id}')\n                        s = s + 1\n                        continue\n\n                    book_data['paragraphs'].append(convert_question_pair_to_squad_format(qa_pair))\n                    passages_to_write[qa_pair.question] = related_passages\n                    data_split = qa_pair.set_split\n\n                json_question_pairs = json.dumps(passages_to_write)\n                fq = open(f\"./document_qa_passages/{document_id}.q_passages\", \"w\")\n                fq.write(json_question_pairs)\n                fq.close()\n\n                set_data_split(data, book_data, data_split, train_data, valid_data, test_data)\n                \n                if max_stories != -1 and i > max_stories:\n                    break\n\n            except Exception:\n                traceback.print_exc()\n                print(f\"Error processing qa pairs and passages for document {document_id}. Story no {i}\")\n                break\n\n        save_train_valid_test_data(data, train_data, valid_data, test_data)\n        \n        print(f'total question pairs: {q}')\n        print(f'skipped pairs: {s}')\n"},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["total question pairs: 46556\n","skipped pairs: 750\n"]}],"source":"#document_questions = get_question_answer_pairs()\nget_and_write_qa_passages_as_squad(document_questions, max_stories=-1)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"#TODO\n\n#try different combinations of characters\n#break up by paragraphs?\n    #heuristic to aim for size?\n\n#probability of passage being correct?\n\n#catalog sparknotes urls"},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["26\n"]}],"source":"text = get_document_text(\"1aae28477e771b3af008ec59ce29086a1bc66776\")\ntest = text.split(\"\\n\\n\")\nprint(len(test))\n#print(test)\n#print(text)"},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":"#to uplaod training data\n#gsutil cp ./squad_document_qa_passages/train.data gs://gutenberg-qa-train-data/train.data"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}