{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_pointer_generator.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjjlYqTXwf7U",
        "colab_type": "text"
      },
      "source": [
        "# Transformer Pointer Generator Model for Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JjJJyJTZYebt",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIFMr_0Or3qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from importlib import reload\n",
        "\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "d_model = 1024 #300 for GloVe embeddings\n",
        "MAX_VOCAB_SIZE = 2**14\n",
        "MAX_Q_LEN = 30\n",
        "MAX_C_LEN = 500\n",
        "MAX_A_LEN = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9CJLqv8wyPN",
        "colab_type": "text"
      },
      "source": [
        "### File Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFf4HrvorCst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone https://github.com/ahathaway821/w266-gutenberg-quiz\n",
        "# !mv /content/w266-gutenberg-quiz /content/w266_gutenberg_quiz\n",
        "#!git -C ./w266_gutenberg_quiz reset --hard\n",
        "#!git -C ./w266_gutenberg_quiz pull origin master\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzIdc71_68zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# project_id ='ace-element-251203'\n",
        "# !gcloud config set project {project_id}\n",
        "# !gsutil cp gs://gutenberg-qa-train-data/train.data /content/mini_train.data\n",
        "# !gsutil cp gs://gutenberg-qa-train-data/train.data /content/train.data\n",
        "# !gsutil cp gs://gutenberg-qa-train-data/train.data /content/valid.data\n",
        "# !gsutil cp gs://gutenberg-qa-train-data/train.data /content/test.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlVr8ik_v8OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from w266_gutenberg_quiz.library.models.seq2seq_att import Seq2SeqAtt\n",
        "#qa = Seq2SeqAtt()\n",
        "#qa.load_glove_model('w266_gutenberg_quiz/embeddings')\n",
        "#qa.load_model(model_dir_path='w266_gutenberg_quiz/experiments/models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz167jEoLZj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "\n",
        "# GLOVE_DIR ='/content/w266_gutenberg_quiz/embeddings'\n",
        "# #d_model = 300\n",
        "\n",
        "# #vocabulary_set = set()\n",
        "# embeddings_index = {}\n",
        "# i = 0\n",
        "# with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')) as f:\n",
        "#     for line in f:\n",
        "#         word, coefs = line.split(maxsplit=1)\n",
        "#         coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "#         embeddings_index[word] = coefs\n",
        "#         # if i < MAX_VOCAB_SIZE:\n",
        "#         #   vocabulary_set.add(word)\n",
        "#         i = i + 1\n",
        "# print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJt2fz-OLfp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRUR2wphLn1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLA0WQTEO6aU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FysGOjs-rUoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from w266_gutenberg_quiz.library.utility.squad_v3 import SquADDataSetV3\n",
        "\n",
        "data_set = SquADDataSetV3(data_path=\"w266_gutenberg_quiz/data/SQuAD/train-v1.1.json\")\n",
        "\n",
        "test_size=.2\n",
        "random_state=42\n",
        "question_train, question_val, context_train, context_val, answer_train, answer_val = train_test_split(data_set.questions, \n",
        "                                                                                                      data_set.contexts, \n",
        "                                                                                                      data_set.answers, \n",
        "                                                                                                      test_size=test_size, \n",
        "                                                                                                      random_state=random_state)\n",
        "train_examples = tf.data.Dataset.from_tensor_slices((question_train, context_train, answer_train))\n",
        "val_examples = tf.data.Dataset.from_tensor_slices((question_val, context_val, answer_val))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6FTYF16Fjbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_set = set()\n",
        "tokenizer = tfds.features.text.Tokenizer()\n",
        "\n",
        "v = 0\n",
        "for question, context, answer in train_examples:\n",
        "  some_tokens = tokenizer.tokenize((question + \" \" + context + \" \" + answer).numpy())\n",
        "  v = v + 1\n",
        "  if len(vocabulary_set) > MAX_VOCAB_SIZE:\n",
        "    break\n",
        "  vocabulary_set.update(some_tokens)\n",
        "  \n",
        "tokenizer_all = tfds.features.text.TokenTextEncoder(vocabulary_set)\n",
        "tokenizer_all.load_from_file(\"/content/drive/My Drive/W266/FInalProject/transformer_v7_vocab/l_squad_elmo_4l_b64_lvocab_slr\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAhpZUX40yYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHvV4R9ChQDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Due to GPU memory constraints in Colab, need to set hard limits on lengths \n",
        "filtered_question_train = []\n",
        "filtered_question_val = []\n",
        "filtered_context_train = []\n",
        "filtered_context_val = []\n",
        "filtered_answer_train = []\n",
        "filtered_answer_val = []\n",
        "\n",
        "max_answer_char_length = 50\n",
        "max_question_char_length = 200\n",
        "max_context_char_length = 2500\n",
        "empty_a = 0\n",
        "for i in range(0, len(question_train)):\n",
        "  if len(answer_train[i]) < max_answer_char_length and len(question_train[i]) < max_question_char_length and len(context_train[i]) < max_context_char_length:\n",
        "    if len(answer_train[i]) == 0:\n",
        "      empty_a = empty_a + 1\n",
        "      continue\n",
        "\n",
        "    filtered_question_train.append(re.sub(r'([^\\s\\w]|_)+', '', question_train[i]))\n",
        "    filtered_context_train.append(re.sub(r'([^\\s\\w]|_)+', '', context_train[i]))\n",
        "    filtered_answer_train.append(re.sub(r'([^\\s\\w]|_)+', '', answer_train[i]))\n",
        "\n",
        "for i in range(0, len(question_val)):\n",
        "  if len(answer_val[i]) < max_answer_char_length and len(question_val[i]) < max_question_char_length and len(context_val[i]) < max_context_char_length:\n",
        "    if len(answer_val[i]) == 0:\n",
        "      continue\n",
        "    filtered_question_val.append(re.sub(r'([^\\s\\w]|_)+', '',question_val[i]))\n",
        "    filtered_context_val.append(re.sub(r'([^\\s\\w]|_)+', '',context_val[i]))\n",
        "    filtered_answer_val.append(re.sub(r'([^\\s\\w]|_)+', '', answer_val[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGEhN9hjSPm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert text into encoded ids with the tokenizer\n",
        "def encoder_text_to_ids(question_words, tokenizer):\n",
        "  ids = []\n",
        "  oovs = []\n",
        "  unk_id = tokenizer_all.vocab_size-1\n",
        "  for w in question_words:\n",
        "    i = tokenizer.encode(w)[0]\n",
        "    if i == unk_id: # If w is OOV\n",
        "      if w not in oovs: # Add to list of OOVs\n",
        "        oovs.append(w)\n",
        "      oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
        "      ids.append(tokenizer.vocab_size + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
        "    else:\n",
        "      ids.append(i)\n",
        "  return ids, oovs\n",
        "\n",
        "def answer_to_ids(answer_words, tokenizer, context_oovs):\n",
        "  ids = []\n",
        "  unk_id = tokenizer_all.vocab_size-1\n",
        "  for w in answer_words:\n",
        "    i = tokenizer.encode(w)[0]\n",
        "    if i == unk_id: # If w is an OOV word\n",
        "      if w in context_oovs: # If w is an in-context OOV\n",
        "        vocab_idx = tokenizer.vocab_size + context_oovs.index(w) # Map to its temporary context OOV number\n",
        "        ids.append(vocab_idx)\n",
        "      else: # If w is an out-of-context OOV\n",
        "        ids.append(unk_id) # Map to the UNK token id\n",
        "    else:\n",
        "      ids.append(i)\n",
        "  return ids\n",
        "\n",
        "def get_dec_inp_targ_seqs(sequence, max_len, start_id, stop_id):\n",
        "  \"\"\"\n",
        "    Given the reference summary as a sequence of tokens, return the input sequence for the decoder, and the target sequence which we will use to calculate loss. The sequence will be truncated if it is longer than max_len. The input sequence must start with the start_id and the target sequence must end with the stop_id (but not if it's been truncated).\n",
        "    Args:\n",
        "      sequence: List of ids (integers)\n",
        "      max_len: integer\n",
        "      start_id: integer\n",
        "      stop_id: integer\n",
        "    Returns:\n",
        "      inp: sequence length <=max_len starting with start_id\n",
        "      target: sequence same length as input, ending with stop_id only if there was no truncation\n",
        "  \"\"\"\n",
        "  inp = [start_id] + sequence[:]\n",
        "  target = sequence[:]\n",
        "  if len(inp) > max_len: # truncate\n",
        "    inp = inp[:max_len]\n",
        "    target = target[:max_len] # no end_token\n",
        "  else: # no truncation\n",
        "    target.append(stop_id) # end token\n",
        "  assert len(inp) == len(target)\n",
        "  return inp, target\n",
        "\n",
        "def add_padding(x, max_length, padding_value=0):\n",
        "  while len(x) < max_length:\n",
        "    x.append(padding_value)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQoxFdiTBvvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def example_generator(parsed_dataset, max_q_enc_len=40, max_c_enc_len=500, max_dec_len=10, training=False):\n",
        "  for q,c,a in parsed_dataset:\n",
        "\n",
        "    question = q.decode()\n",
        "    context = c.decode()\n",
        "    answer =a.decode()\n",
        "\n",
        "    start_decoding = tokenizer_all.vocab_size\n",
        "    stop_decoding = tokenizer_all.vocab_size + 1\n",
        "\n",
        "    question_words = question.split()[ : max_q_enc_len]\n",
        "    enc_q_len = len(question_words)\n",
        "    enc_q_input = [tokenizer_all.encode(w)[0] for w in question_words]\n",
        "\n",
        "    enc_q_input_extend_vocab, question_oovs = encoder_text_to_ids(question_words, tokenizer_all)\n",
        "    enc_q_input = enc_q_input\n",
        "\n",
        "    context_words = context.split()[ : max_c_enc_len]\n",
        "    enc_c_len = len(context_words)\n",
        "    enc_c_input = [tokenizer_all.encode(w)[0] for w in context_words]\n",
        "    enc_c_input_extend_vocab, context_oovs = encoder_text_to_ids(context_words, tokenizer_all)\n",
        "    enc_c_input = enc_c_input# + [stop_decoding]\n",
        "\n",
        "    answer_words = answer.split()[ : max_dec_len-2]\n",
        "    answer_ids = [tokenizer_all.encode(w)[0] for w in answer_words]\n",
        "    answer_ids_extend_vocab = answer_to_ids(answer_words, tokenizer_all, context_oovs)\n",
        "\n",
        "    dec_input = [start_decoding] + answer_ids + [stop_decoding]\n",
        "    target = dec_input\n",
        "    dec_len = max_dec_len-1#len(dec_input) - 1\n",
        "    answer_words = [\"<s>\"] + answer_words\n",
        "\n",
        "    output = {\n",
        "      \"enc_q_len\": enc_q_len,\n",
        "      \"enc_q_input\" : enc_q_input,\n",
        "      \"enc_q_input_extend_vocab\"  : enc_q_input_extend_vocab,\n",
        "      \"question_oovs\" : question_oovs,\n",
        "      \"question_words\" : question_words,\n",
        "      \"enc_c_len\": enc_c_len,\n",
        "      \"enc_c_input\" : enc_c_input,\n",
        "      \"enc_c_input_extend_vocab\"  : enc_c_input_extend_vocab,\n",
        "      \"context_words\": context_words,\n",
        "      \"context_oovs\" : context_oovs,\n",
        "      \"dec_input\" : dec_input,\n",
        "      \"target\" : target,\n",
        "      \"dec_len\" : dec_len,\n",
        "      \"answer_words\" : answer_words\n",
        "    }\n",
        "\n",
        "    yield output\n",
        "\n",
        "\n",
        "def batch_generator(generator, parsed_dataset, max_q_enc_len=MAX_Q_LEN, max_c_enc_len=MAX_C_LEN, max_dec_len=MAX_A_LEN, batch_size=BATCH_SIZE, training=True):\n",
        "  dataset = tf.data.Dataset.from_generator(generator, args = [parsed_dataset, max_q_enc_len, max_c_enc_len, max_dec_len, training],\n",
        "                      output_types = {\n",
        "                        \"enc_q_len\": tf.int32,\n",
        "                        \"enc_q_input\" : tf.int32,\n",
        "                        \"enc_q_input_extend_vocab\" : tf.int32,\n",
        "                        \"question_oovs\" : tf.string,\n",
        "                        \"question_words\" : tf.string,\n",
        "                        \"enc_c_len\":tf.int32,\n",
        "                        \"enc_c_input\" : tf.int32,\n",
        "                        \"enc_c_input_extend_vocab\" : tf.int32,\n",
        "                        \"context_oovs\" : tf.string,\n",
        "                        \"context_words\": tf.string,\n",
        "                        \"dec_input\" : tf.int32,\n",
        "                        \"target\" : tf.int32,\n",
        "                        \"dec_len\" : tf.int32,\n",
        "                        \"answer_words\": tf.string\n",
        "                      }, output_shapes={\n",
        "                        \"enc_q_len\": [],\n",
        "                        \"enc_q_input\" : [None],\n",
        "                        \"enc_q_input_extend_vocab\" : [None],\n",
        "                        \"question_oovs\" : [None],\n",
        "                        \"question_words\": [None],\n",
        "                        \"enc_c_len\":[],\n",
        "                        \"enc_c_input\" : [None],\n",
        "                        \"enc_c_input_extend_vocab\" : [None],\n",
        "                        \"context_words\": [None],\n",
        "                        \"context_oovs\" : [None],\n",
        "                        \"dec_input\" : [None],\n",
        "                        \"target\" : [None],\n",
        "                        \"dec_len\" : [],\n",
        "                        \"answer_words\": [None]\n",
        "                      })\n",
        "  dataset = dataset.padded_batch(batch_size, padded_shapes=({\n",
        "                        \"enc_q_len\": [],\n",
        "                        \"enc_q_input\" : [None],\n",
        "                        \"enc_q_input_extend_vocab\" : [None],\n",
        "                        \"question_oovs\" : [None],\n",
        "                        \"question_words\": [None],\n",
        "                        \"enc_c_len\":[],\n",
        "                        \"enc_c_input\" : [None],\n",
        "                        \"enc_c_input_extend_vocab\"  : [None],\n",
        "                        \"context_oovs\" : [None],\n",
        "                        \"context_words\": [None],\n",
        "                        \"dec_input\" : [max_dec_len],\n",
        "                        \"target\" : [max_dec_len],\n",
        "                        \"dec_len\" : [],\n",
        "                        \"answer_words\": [max_dec_len-1]}),\n",
        "                      padding_values={\n",
        "                        \"enc_q_len\": -1,\n",
        "                        \"enc_q_input\" : 0,\n",
        "                        \"enc_q_input_extend_vocab\" : 0,\n",
        "                        \"question_oovs\" : b'',\n",
        "                        \"question_words\": b'',\n",
        "                        \"enc_c_len\":-1,\n",
        "                        \"enc_c_input\" : 0,\n",
        "                        \"enc_c_input_extend_vocab\"  : 0,\n",
        "                        \"context_oovs\" : b'',\n",
        "                        \"context_words\": b'',\n",
        "                        \"dec_input\" : 0,\n",
        "                        \"target\" : 0,\n",
        "                        \"dec_len\" : -1,\n",
        "                        \"answer_words\": b''\n",
        "                      },\n",
        "                      drop_remainder=True)\n",
        "  def update(entry):\n",
        "    return ({\"enc_q_input\" : entry[\"enc_q_input\"],\n",
        "      \"extended_enc_q_input\" : entry[\"enc_q_input_extend_vocab\"],\n",
        "      \"question_oovs\" : entry[\"question_oovs\"],\n",
        "      \"enc_q_len\" : entry[\"enc_q_len\"],\n",
        "      \"max_q_oov_len\" : tf.shape(entry[\"question_oovs\"])[1],\n",
        "      \"question_tokens\": entry[\"question_words\"] },\n",
        "\n",
        "      {\"enc_c_input\" : entry[\"enc_c_input\"],\n",
        "      \"extended_enc_c_input\" : entry[\"enc_c_input_extend_vocab\"],\n",
        "      \"context_oovs\" : entry[\"context_oovs\"],\n",
        "      \"enc_c_len\" : entry[\"enc_c_len\"],\n",
        "      \"max_c_oov_len\" : tf.shape(entry[\"context_oovs\"])[1],\n",
        "      \"context_tokens\": entry[\"context_words\"]},\n",
        "\n",
        "      {\"dec_input\" : entry[\"dec_input\"],\n",
        "      \"dec_target\" : entry[\"target\"],\n",
        "      \"dec_len\" : entry[\"dec_len\"],\n",
        "      \"answer_tokens\": entry[\"answer_words\"]})\n",
        "\n",
        "\n",
        "  dataset = dataset.map(update)\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfKbGgIRZ5Jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = batch_generator(example_generator, np.column_stack((filtered_question_train, filtered_context_train, filtered_answer_train)), training=True)\n",
        "val_dataset = batch_generator(example_generator, np.column_stack((filtered_question_val, filtered_context_val, filtered_answer_val)) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fkoaHZBvRnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prefetch the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wd6kNP4SxUJP"
      },
      "source": [
        "### Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0UiybnSBzfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Rz82wEs5biZ",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a_b4ou4TYqUN"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s42Uydjkv0hF"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U2i8-e1s8ti9",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "# def create_padding_mask(seq):\n",
        "#   seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "#   # add extra dimensions to add the padding\n",
        "#   # to the attention logits.\n",
        "#   return seq[:, tf.newaxis, :]  # (batch_size, 1, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0hzukDBgVom"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dVxS8OPI9uI0",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7iEY3opXqfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(q, c, a):\n",
        "  # Encoder padding mask\n",
        "  q_enc_padding_mask = create_padding_mask(q)\n",
        "  c_enc_padding_mask = create_padding_mask(c)\n",
        "\n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  q_dec_padding_mask = create_padding_mask(q)\n",
        "  c_dec_padding_mask = create_padding_mask(c)\n",
        "\n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(a)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(a)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "  \n",
        "  return q_enc_padding_mask, c_enc_padding_mask, combined_mask, q_dec_padding_mask, c_dec_padding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xluDl5cXYy4y"
      },
      "source": [
        "## Scaled dot product attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LazzUq3bJ5SH",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention_v1(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask, extra=False):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention_v1(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FiqETnhCkoXh"
      },
      "source": [
        "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
        "\n",
        "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kmzGPEy64qmA"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCgdRsfKReNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention_v2(Q, K, V,\n",
        "                                 num_heads,\n",
        "                                 mask,\n",
        "                                 causality=False, dropout_rate=0.,\n",
        "                                 training=True,\n",
        "                                 scope=\"scaled_dot_product_attention\"):\n",
        "    '''See 3.2.1.\n",
        "    Q: Packed queries. 3d tensor. [N, T_q, d_k].\n",
        "    K: Packed keys. 3d tensor. [N, T_k, d_k].\n",
        "    V: Packed values. 3d tensor. [N, T_k, d_v].\n",
        "    causality: If True, applies masking for future blinding\n",
        "    dropout_rate: A floating point number of [0, 1].\n",
        "    training: boolean for controlling droput\n",
        "    scope: Optional scope for `variable_scope`.\n",
        "    '''\n",
        "    dk = Q.get_shape().as_list()[-1]\n",
        "\n",
        "    # dot product\n",
        "    outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (N, T_q, T_k)\n",
        "\n",
        "    # scale\n",
        "    outputs /= dk ** 0.5\n",
        "\n",
        "    # causality or future blinding masking\n",
        "    if causality:\n",
        "      outputs = f_mask(outputs, type=\"future\")\n",
        "\n",
        "    # softmax\n",
        "    attn_dists = tf.nn.softmax(tf.reduce_sum(tf.split(outputs, num_heads, axis=0), axis=0))\n",
        "    outputs = tf.nn.softmax(outputs)\n",
        "    attention = tf.transpose(outputs, [0, 2, 1])\n",
        "\n",
        "    # weighted sum (context vectors)\n",
        "    outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n",
        "\n",
        "    return outputs, attn_dists\n",
        "\n",
        "class MultiHeadAttention_v2(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention_v2, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask, causality=False):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    num_heads=4\n",
        "    Q_ = tf.concat(tf.split(q, num_heads, axis=2), axis=0) # (h*N, T_q, d_model/h)\n",
        "    K_ = tf.concat(tf.split(k, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
        "    V_ = tf.concat(tf.split(v, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
        "\n",
        "    # Attention\n",
        "    dropout_rate=.1\n",
        "    training=True\n",
        "    outputs, attn_dists = scaled_dot_product_attention_v2(Q_, K_, V_, num_heads, mask, causality)\n",
        "\n",
        "    # Restore shape\n",
        "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2) # (N, T_q, d_model)\n",
        "    # Residual connection\n",
        "    #outputs = queries + outputs\n",
        "          \n",
        "    # Normalize\n",
        "    #outputs = ln(outputs)\n",
        " \n",
        "    return outputs, attn_dists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RdDqGayx67vv"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gBqzJXGfHK3X"
      },
      "source": [
        "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ET7xLt0yCT6Z",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50TKZQMXP8h-",
        "colab_type": "text"
      },
      "source": [
        "## ELMo Embedding\n",
        "Unfortunately, ELMo the 4 available training layers for ELMo are not currently supported in TF2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VvZoFU7P7Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# #import hub\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "embed_input_sig = [\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, None), dtype=tf.string), #q input\n",
        "    tf.TensorSpec(shape=(None, ), dtype=tf.int32), #q extended input\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, None), dtype=tf.string), #c input\n",
        "    tf.TensorSpec(shape=(None, ), dtype=tf.int32), #c extended input\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, None), dtype=tf.string), # q embed\n",
        "    tf.TensorSpec(shape=(None, ), dtype=tf.int32), # c embed\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=embed_input_sig)\n",
        "def get_elmo_embeddings(question_tokens, q_len, context_tokens, c_len, tar_tokens ,tar_len):\n",
        "    q_embed = elmo_embed_layer({\"tokens\": question_tokens, \"sequence_len\": q_len})\n",
        "    c_embed = elmo_embed_layer({\"tokens\": context_tokens, \"sequence_len\": c_len })\n",
        "    tar_embed = elmo_embed_layer({\"tokens\": tar_tokens, \"sequence_len\": tar_len})\n",
        "\n",
        "    return q_embed, c_embed, tar_embed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIt-mazLGX5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elmo_embed_layer = hub.KerasLayer('https://tfhub.dev/google/elmo/3', signature=\"tokens\", trainable=False, output_key=\"elmo\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7e7hKcxn6-zd"
      },
      "source": [
        "## Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfYJG-Kvgwy2"
      },
      "source": [
        "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
        "\n",
        "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
        "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFv-FNYUmvpn"
      },
      "source": [
        "### Encoder layer\n",
        "\n",
        "Each encoder layer consists of sublayers:\n",
        "\n",
        "1.   Multi-head attention (with padding mask) \n",
        "2.    Point wise feed forward networks. \n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
        "\n",
        "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ncyS-Ms3i2x_",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "    attn_output, att_weights = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2, att_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AzZRXdO0mI48",
        "colab": {}
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(d_model, 4, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, d_model)), False, None)\n",
        "\n",
        "print(sample_encoder_layer_output[0].shape)  # (batch_size, input_seq_len, d_model)\n",
        "print(sample_encoder_layer_output[1].shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6LO_48Owmx_o"
      },
      "source": [
        "### Decoder layer\n",
        "\n",
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
        "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
        "3.   Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
        "\n",
        "There are N decoder layers in the transformer.\n",
        "\n",
        "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9SoX0-vd1hue",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha3 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm4 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout4 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output_q, enc_output_c, training, \n",
        "           look_ahead_mask, padding_mask_q, padding_mask_c):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask, True)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(enc_output_q, enc_output_q, out1, padding_mask_q)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    attn3, attn_weights_block3 = self.mha3(enc_output_c, enc_output_c, out2, padding_mask_c)  # (batch_size, target_seq_len, d_model)\n",
        "    attn3 = self.dropout2(attn3, training=training)\n",
        "    out3 = self.layernorm2(attn3 + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out3)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out4 = self.layernorm3(ffn_output + out3)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out4, attn_weights_block2, attn_weights_block3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ne2Bqx8k71l0",
        "colab": {}
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(d_model, 4, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, d_model)), sample_encoder_layer_output[0], sample_encoder_layer_output[1],\n",
        "    False, None, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyI9LPSTw9hC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deprecated\n",
        "class PointerDecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, vocab_size, rate=0.1):\n",
        "    super(PointerDecoderLayer, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.mha1 = MultiHeadAttention_v2(d_model, num_heads)\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    #self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.gen_layer = tf.keras.layers.Dense(1, activation=tf.sigmoid, trainable=True, use_bias=False)\n",
        "\n",
        "\n",
        "  def call(self, x, before_dec, enc_input, enc_output_q, enc_output_c, training=False):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "    attn1, attn_weights_block1 = self.mha1(enc_output_q, enc_output_c, x, None, True)  # (batch_size, target_seq_len, d_model)\n",
        "    #attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, target_seq_len, d_model)\n",
        "    #ffn_output = self.dropout1(ffn_output, training=training)\n",
        "    x = self.layernorm1(ffn_output + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    weights = tf.transpose(embeddingLayer.weights[0]) # (d_model, vocab_size)\n",
        "    logits = tf.einsum('ntd,dk->ntk', x, weights) # (N, T2, vocab_size)\n",
        "    gens = self.gen_layer(tf.concat([before_dec, x, attn_weights_block1], axis=-1))\n",
        "    logits = tf.nn.softmax(logits)\n",
        "    logits = calc_final_dist(enc_input, gens, logits, attn_weights_block1, self.vocab_size)\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SE1H51Ajm0q1"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "The `Encoder` consists of:\n",
        "1.   Input Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N encoder layers\n",
        "\n",
        "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kACxrAJ6wj7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Similarity(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Similarity, self).__init__(**kwargs)\n",
        "\n",
        "    def compute_similarity(self, repeated_context_vectors, repeated_query_vectors):\n",
        "        element_wise_multiply = repeated_context_vectors * repeated_query_vectors\n",
        "        concatenated_tensor = tf.keras.backend.concatenate(\n",
        "            [repeated_context_vectors, repeated_query_vectors, element_wise_multiply], axis=-1)\n",
        "        dot_product = tf.squeeze(tf.keras.backend.dot(concatenated_tensor, self.kernel), axis=-1)\n",
        "        return tf.keras.activations.linear(dot_product + self.bias)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        word_vector_dim = input_shape[0][-1]\n",
        "        weight_vector_dim = word_vector_dim * 3\n",
        "        self.kernel = self.add_weight(name='similarity_weight',\n",
        "                                      shape=(weight_vector_dim, 1),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        self.bias = self.add_weight(name='similarity_bias',\n",
        "                                    shape=(),\n",
        "                                    initializer='ones',\n",
        "                                    trainable=True)\n",
        "        super(Similarity, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        context_vectors, query_vectors = inputs\n",
        "        num_context_words = tf.shape(context_vectors)[1]\n",
        "        num_query_words = tf.shape(query_vectors)[1]\n",
        "        context_dim_repeat = tf.keras.backend.concatenate([[1, 1], [num_query_words], [1]], 0)\n",
        "        query_dim_repeat = tf.keras.backend.concatenate([[1], [num_context_words], [1, 1]], 0)\n",
        "        repeated_context_vectors = tf.tile(tf.expand_dims(context_vectors, axis=2), context_dim_repeat)\n",
        "        repeated_query_vectors = tf.tile(tf.expand_dims(query_vectors, axis=1), query_dim_repeat)\n",
        "        similarity_matrix = self.compute_similarity(repeated_context_vectors, repeated_query_vectors)\n",
        "        return similarity_matrix\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        batch_size = input_shape[0][0]\n",
        "        num_context_words = input_shape[0][1]\n",
        "        num_query_words = input_shape[1][1]\n",
        "        return (batch_size, num_context_words, num_query_words)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class C2QAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(C2QAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(C2QAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        similarity_matrix, encoded_question = inputs\n",
        "        context_to_query_attention = tf.keras.layers.Softmax(axis=-1)(similarity_matrix)\n",
        "        encoded_question = tf.expand_dims(encoded_question, axis=1)\n",
        "        return tf.keras.backend.sum(tf.expand_dims(context_to_query_attention, axis=-1) * encoded_question, -2)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        similarity_matrix_shape, encoded_question_shape = input_shape\n",
        "        return similarity_matrix_shape[:-1] + encoded_question_shape[-1:]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "class Q2CAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Q2CAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Q2CAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        similarity_matrix, encoded_context = inputs\n",
        "        max_similarity = tf.keras.backend.max(similarity_matrix, axis=-1)\n",
        "        # by default, axis = -1 in Softmax\n",
        "        context_to_query_attention = tf.keras.layers.Softmax()(max_similarity)\n",
        "        weighted_sum = tf.keras.backend.sum(tf.expand_dims(context_to_query_attention, axis=-1) * encoded_context, -2)\n",
        "        expanded_weighted_sum = tf.expand_dims(weighted_sum, 1)\n",
        "        num_of_repeatations = tf.shape(encoded_context)[1]\n",
        "        return tf.tile(expanded_weighted_sum, [1, num_of_repeatations, 1])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        similarity_matrix_shape, encoded_context_shape = input_shape\n",
        "        return similarity_matrix_shape[:-1] + encoded_context_shape[-1:]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV2C8ijY0YTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddingLayer = tf.keras.layers.Embedding(tokenizer_all.vocab_size + 2, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9tvhyxJS0Sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder_QA(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               q_maximum_position_encoding, c_maximum_position_encoding, rate=0.1, num_q_model_layers=2, num_c_model_layers=2):\n",
        "    super(Encoder_QA, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.num_q_model_layers = num_q_model_layers\n",
        "    self.num_c_model_layers = num_c_model_layers\n",
        "    \n",
        "    self.q_pos_encoding = positional_encoding(q_maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    self.c_pos_encoding = positional_encoding(c_maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    self.q_enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.c_enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, q, c, training, q_mask, c_mask):\n",
        "    \n",
        "\n",
        "    q_seq_len = tf.shape(q)[1]\n",
        "    c_seq_len = tf.shape(c)[1]\n",
        "\n",
        "    q = self.dropout(q, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      q, q_concat_att = self.q_enc_layers[i](q, training, q_mask)\n",
        "\n",
        "    c = self.dropout(c, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      c, c_concat_att  = self.c_enc_layers[i](c, training, c_mask)\n",
        "    \n",
        "    return q, c, q_concat_att, c_concat_att  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8QG9nueFQKXx",
        "colab": {}
      },
      "source": [
        "sample_encoder = Encoder_QA(num_layers=2, d_model=d_model, num_heads=4, \n",
        "                         dff=2048, vocab_size=8500,\n",
        "                         q_maximum_position_encoding=10000, c_maximum_position_encoding=10000)\n",
        "enc_temp_input = tf.random.uniform((64, 62), dtype=tf.int32, minval=0, maxval=200)\n",
        "enc_temp_input_2 = tf.random.uniform((64, 100), dtype=tf.int32, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(embeddingLayer(enc_temp_input), embeddingLayer(enc_temp_input_2), training=False, q_mask=None, c_mask=None)\n",
        "\n",
        "print (sample_encoder_output[0].shape)  # (batch_size, input_seq_len, d_model)\n",
        "print (sample_encoder_output[1].shape)  # (batch_size, input_seq_len, d_model)\n",
        "print (sample_encoder_output[3].shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCn16gjulmO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-uO6ls8m2O5"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZtT7PKzrXkNr"
      },
      "source": [
        " The `Decoder` consists of:\n",
        "1.   Output Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N decoder layers\n",
        "\n",
        "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PApG6s60eInu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder_QA(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder_QA, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = target_vocab_size\n",
        "    self.num_heads=num_heads\n",
        "    self.depth = self.d_model // self.num_heads\n",
        "\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers= [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    self.Wh = tf.keras.layers.Dense(1)\n",
        "    self.Ws = tf.keras.layers.Dense(1)\n",
        "    self.Wx = tf.keras.layers.Dense(1)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, x, enc_output, enc_input, training, \n",
        "           look_ahead_mask, q_padding_mask, c_padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    before_x = x\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      # Query encoder ouput\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output[0], enc_output[1], training,\n",
        "                                              look_ahead_mask, q_padding_mask, c_padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "    #context vectors\n",
        "    enc_out_shape = tf.shape(enc_output[1])\n",
        "    context = tf.reshape(enc_output[1],(enc_out_shape[0], enc_out_shape[1], self.num_heads, self.depth) ) # shape : (batch_size, input_seq_len, num_heads, depth)\n",
        "    context = tf.transpose(context, [0,2,1,3]) # (batch_size, num_heads, input_seq_len, depth)\n",
        "    context = tf.expand_dims(context, axis=2)  # (batch_size, num_heads, 1, input_seq_len, depth)\n",
        "\n",
        "    attn = tf.expand_dims(block2, axis=-1)  # (batch_size, num_heads, target_seq_len, input_seq_len, 1)\n",
        "\n",
        "    context = context * attn # (batch_size, num_heads, target_seq_len, input_seq_len, depth)\n",
        "    context = tf.reduce_sum(context, axis=3) # (batch_size, num_heads, target_seq_len, depth)\n",
        "    context = tf.transpose(context, [0,2,1,3]) # (batch_size, target_seq_len, num_heads, depth)\n",
        "    context = tf.reshape(context, (tf.shape(context)[0], tf.shape(context)[1], self.d_model)) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    #P_gens computing\n",
        "    a = self.Wx(before_x)\n",
        "    b = self.Ws(x)\n",
        "    c = self.Wh(context)\n",
        "    p_gens = tf.sigmoid(self.V(a + b + c))\n",
        "\n",
        "    return x, attention_weights, p_gens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1jXoAMRZyvu",
        "colab": {}
      },
      "source": [
        "sample_decoder = Decoder_QA(num_layers=4, d_model=d_model, num_heads=4, \n",
        "                         dff=2048, target_vocab_size=tokenizer_all.vocab_size +2 ,\n",
        "                         maximum_position_encoding=5000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int32, minval=0, maxval=200)\n",
        "\n",
        "output, attn, p_gens = sample_decoder(embeddingLayer(temp_input), \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              enc_input=embeddingLayer(enc_temp_input_2), \n",
        "                              training=False,\n",
        "                              look_ahead_mask=None, \n",
        "                              q_padding_mask=None,\n",
        "                              c_padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape\n",
        "\n",
        "# Currently trying to align encoder input shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyJPPesPuhr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _calc_final_dist( _enc_batch_extend_vocab, vocab_dists, attn_dists, p_gens, batch_oov_len, vocab_size, batch_size):\n",
        "  \"\"\"Calculate the final distribution, for the pointer-generator model\n",
        "  Args:\n",
        "  vocab_dists: The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays. The words are in the order they appear in the vocabulary file.\n",
        "  attn_dists: The attention distributions. List length max_dec_steps of (batch_size, attn_len) arrays\n",
        "  Returns:\n",
        "  final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vsize) arrays.\n",
        "  \"\"\"\n",
        "  # Multiply vocab dists by p_gen and attention dists by (1-p_gen)\n",
        "  vocab_dists = [p_gen * dist for (p_gen,dist) in zip(p_gens, vocab_dists)]\n",
        "  attn_dists = [(1-p_gen) * dist for (p_gen,dist) in zip(p_gens, attn_dists)]\n",
        "\n",
        "  # Concatenate some zeros to each vocabulary dist, to hold the probabilities for in-article OOV words\n",
        "  extended_vsize = vocab_size + batch_oov_len # the maximum (over the batch) size of the extended vocabulary\n",
        "  extra_zeros = tf.zeros((batch_size, batch_oov_len ))\n",
        "  vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists] # list length max_dec_steps of shape (batch_size, extended_vsize)\n",
        "\n",
        "  # Project the values in the attention distributions onto the appropriate entries in the final distributions\n",
        "  # This means that if a_i = 0.1 and the ith encoder word is w, and w has index 500 in the vocabulary, then we add 0.1 onto the 500th entry of the final distribution\n",
        "  # This is done for each decoder timestep.\n",
        "  # This is fiddly; we use tf.scatter_nd to do the projection\n",
        "  batch_nums = tf.range(0, limit=batch_size) # shape (batch_size)\n",
        "  batch_nums = tf.expand_dims(batch_nums, 1) # shape (batch_size, 1)\n",
        "  attn_len = tf.shape(_enc_batch_extend_vocab)[1] # number of states we attend over\n",
        "  batch_nums = tf.tile(batch_nums, [1, attn_len]) # shape (batch_size, attn_len)\n",
        "  indices = tf.stack( (batch_nums, _enc_batch_extend_vocab), axis=2) # shape (batch_size, enc_t, 2)\n",
        "  shape = [batch_size, extended_vsize]\n",
        "  attn_dists_projected = [tf.scatter_nd(indices, copy_dist, shape) for copy_dist in attn_dists] # list length max_dec_steps (batch_size, extended_vsize)\n",
        "\n",
        "  # Add the vocab distributions and the copy distributions together to get the final distributions\n",
        "  # final_dists is a list length max_dec_steps; each entry is a tensor shape (batch_size, extended_vsize) giving the final distribution for that decoder timestep\n",
        "  # Note that for decoder timesteps and examples corresponding to a [PAD] token, this is junk - ignore.\n",
        "  final_dists = [vocab_dist + copy_dist for (vocab_dist,copy_dist) in zip(vocab_dists_extended, attn_dists_projected)]\n",
        "\n",
        "  return final_dists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## Create the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uERO1y54cOKq"
      },
      "source": [
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PED3bIpOYkBu",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "                  pe_input_q, pe_input_c, pe_target, embedding_layer, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_layers=num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.encoder = Encoder_QA(num_layers, d_model, num_heads, dff, \n",
        "                           vocab_size, pe_input_q, pe_input_c, rate)\n",
        "\n",
        "    self.decoder = Decoder_QA(num_layers, d_model, num_heads, dff, \n",
        "                           vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "    self.embedding_layer = embedding_layer\n",
        "    \n",
        "    \n",
        "  def call(self, q, c, c_ext, tar, training, q_enc_padding_mask, c_enc_padding_mask,\n",
        "           look_ahead_mask, q_dec_padding_mask, c_dec_padding_mask,                                  \n",
        "           q_embed, c_embed, tar_embed,\n",
        "           max_oov_len=0, batch_size=BATCH_SIZE, ):\n",
        "    #elmo_embed_layer({\"tokens\": q['question_tokens'], \"sequence_len\": q['enc_q_len']})\n",
        "    # embed_q = self.embedding_layer(q)\n",
        "    # embed_c = self.embedding_layer(c)\n",
        "    # embed_tar = self.embedding_layer(tar)\n",
        "    if q_embed is not None:\n",
        "      embed_q = q_embed\n",
        "      embed_c = c_embed\n",
        "      embed_tar = tar_embed\n",
        "    else:\n",
        "      embed_q = self.embedding_layer(q)\n",
        "      embed_c = self.embedding_layer(c)\n",
        "      embed_tar = self.embedding_layer(tar)\n",
        "    \n",
        "\n",
        "    enc_output = self.encoder(embed_q, embed_c, training, q_enc_padding_mask, c_enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    dec_output, attention_weights, p_gens = self.decoder(\n",
        "        embed_tar, enc_output, embed_c, training, look_ahead_mask, q_dec_padding_mask, c_dec_padding_mask)\n",
        "    output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    output = tf.nn.softmax(output) # (batch_size, tar_seq_len, vocab_size)\n",
        "\n",
        "    attn_dists = attention_weights['decoder_layer{}_block2'.format(self.num_layers)] # (batch_size,num_heads, targ_seq_len, inp_seq_len)\n",
        "    attn_dists = tf.reduce_sum(attn_dists, axis=1)/self.num_heads # (batch_size, targ_seq_len, inp_seq_len)\n",
        "\n",
        "    final_dists = _calc_final_dist( c_ext, tf.unstack(output, axis=1) , tf.unstack(attn_dists, axis=1), tf.unstack(p_gens, axis=1), max_oov_len, self.vocab_size, batch_size)\n",
        "    final_output = tf.stack(final_dists, axis=1)\n",
        "\n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tJ4fbQcIkHW1",
        "colab": {}
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=4, d_model=d_model, num_heads=4, dff=2048, \n",
        "    vocab_size=tokenizer_all.vocab_size, pe_input_q=10000, pe_input_c=10000, pe_target=6000, embedding_layer=embeddingLayer)\n",
        "\n",
        "temp_q = tf.random.uniform((BATCH_SIZE, 38), dtype=tf.int32, minval=0, maxval=200)\n",
        "temp_c = tf.random.uniform((BATCH_SIZE, 38), dtype=tf.int32, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((BATCH_SIZE, 36), dtype=tf.int32, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_q, temp_c, temp_c, temp_target, training=False, \n",
        "                               q_enc_padding_mask=None, \n",
        "                               c_enc_padding_mask=None,\n",
        "                               look_ahead_mask=None,\n",
        "                               q_dec_padding_mask=None,\n",
        "                               c_dec_padding_mask=None,\n",
        "                               q_embed = None,\n",
        "                               c_embed = None,\n",
        "                               tar_embed = None,\n",
        "                               batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFVtNN_3yKF7",
        "colab_type": "text"
      },
      "source": [
        "### GloVE Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBvRR7FMVdbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare GloVE embedding matrix\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "# num_words = len(tokenizer_all.tokens)\n",
        "# embedding_matrix = np.zeros((tokenizer_all.vocab_size + 2, d_model))\n",
        "# for i in range(0, num_words-1):\n",
        "#     word = tokenizer_all.tokens[i]\n",
        "\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# # load pre-trained word embeddings into an Embedding layer\n",
        "# # note that we set trainable = False so as to keep the embeddings fixed\n",
        "# embeddingLayer = tf.keras.layers.Embedding(tokenizer_all.vocab_size + 2,\n",
        "#                             d_model,\n",
        "#                             embeddings_initializer=Constant(embedding_matrix),\n",
        "#                             trainable=False)\n",
        "\n",
        "# num_words = len(tokenizer_all.subwords)\n",
        "# embedding_matrix = np.zeros((tokenizer_all.vocab_size + 2, d_model))\n",
        "# for i in range(0, num_words-1):\n",
        "#     word = tokenizer_all.subwords[i]\n",
        "#     if word[-1] == '_':\n",
        "#       word = word[0:-1]\n",
        "\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# # load pre-trained word embeddings into an Embedding layer\n",
        "# # note that we set trainable = False so as to keep the embeddings fixed\n",
        "# embeddingLayer = tf.keras.layers.Embedding(tokenizer_all.vocab_size + 2,\n",
        "#                             d_model,\n",
        "#                             embeddings_initializer=Constant(embedding_matrix),\n",
        "#                             trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lnJn5SLA2ahP",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "dff = 256\n",
        "num_heads = 4\n",
        "vocab_size = tokenizer_all.vocab_size + 2\n",
        "dropout_rate = 0.05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xYEGhEOtzn5W"
      },
      "source": [
        "## Optimizer, Loss, and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iYQdOO1axwEI",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aeHumfr7zmMa"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UiysUa--4tOU",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "#embeddingLayer = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "# embeddingLayer = tf.keras.layers.Embedding(vocab_size,\n",
        "#                                           d_model,\n",
        "#                                           embeddings_initializer=Constant(embedding_matrix),\n",
        "#                                           trainable=False)\n",
        "#elmo_embed_layer = hub.KerasLayer('https://tfhub.dev/google/elmo/3', signature=\"tokens\", trainable=False, output_key=\"elmo\")\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          vocab_size,\n",
        "                          pe_input_q=vocab_size, \n",
        "                          pe_input_c=vocab_size,\n",
        "                          pe_target=vocab_size,\n",
        "                          rate=dropout_rate,\n",
        "                          embedding_layer=elmo_embed_layer)\n",
        "\n",
        "loss = []\n",
        "accuracy = []\n",
        "e = []\n",
        "\n",
        "#checkpoint_path = \"./checkpoint/train3/v11\"\n",
        "checkpoint_path = \"/content/drive/My Drive/W266/FInalProject/transformer_v7_checkpoints/l_squad_elmo_4l_b64_lvocab_slr\"\n",
        "\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "#status = ckpt.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
        "#print(status)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZOJUSB1T8GjM",
        "colab": {}
      },
      "source": [
        "tokenizer_all.save_to_file(\"/content/drive/My Drive/W266/FInalProject/transformer_v7_vocab/l_squad_elmo_4l_b64_lvocab_slr\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hNhuYfllndLZ",
        "colab": {}
      },
      "source": [
        "print(f'MAX_Q_LEN: {MAX_Q_LEN}')\n",
        "print(f'MAX_C_LEN: {MAX_C_LEN}')\n",
        "print(f'MAX_A_LEN: {MAX_A_LEN}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3p6hbaWPhI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LKpoA6q1sJFj",
        "colab": {}
      },
      "source": [
        "EPOCHS = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJwmp9OE29oj",
        "colab": {}
      },
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, None), dtype=tf.int32), #q input\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, None), dtype=tf.int32), #q extended input\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, None), dtype=tf.int32), #c input\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, None), dtype=tf.int32), #c extended input\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32), # c max oov length\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, MAX_A_LEN), dtype=tf.int32), # target input\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, None, d_model), dtype=tf.float32), # q embed\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, None, d_model), dtype=tf.float32), # c embed\n",
        "    tf.TensorSpec(shape=(BATCH_SIZE, MAX_A_LEN-1, d_model), dtype=tf.float32), # tar embed\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(q, q_e, c, c_e, max_c_oov_len, tar_inp_1, q_embed, c_embed, tar_embed):\n",
        "  tar_inp = tar_inp_1[:, :-1] # start token\n",
        "  tar_real = tar_inp_1[:, 1:] # end token\n",
        "\n",
        "  q_enc_padding_mask, c_enc_padding_mask, combined_mask, q_dec_padding_mask, c_dec_padding_mask = create_masks(q, c, tar_inp)\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(q, c, c_e, tar_inp, \n",
        "                                 True, \n",
        "                                 q_enc_padding_mask, \n",
        "                                 c_enc_padding_mask,\n",
        "                                 combined_mask, \n",
        "                                 q_dec_padding_mask,\n",
        "                                 c_dec_padding_mask,\n",
        "                                 q_embed, c_embed, tar_embed,\n",
        "                                 max_oov_len=max_c_oov_len)\n",
        "    \n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)   \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bbvmaKNiznHZ",
        "colab": {}
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  # question, context -> answer\n",
        "  for (batch, (q, c, tar)) in enumerate(train_dataset):\n",
        "    q_embed, c_embed, tar_embed = get_elmo_embeddings(q['question_tokens'], q['enc_q_len'], c['context_tokens'], c['enc_c_len'], tar['answer_tokens'], tar['dec_len'])\n",
        "    train_step(q['enc_q_input'], q['extended_enc_q_input'], c['enc_c_input'], c['extended_enc_c_input'], c['max_c_oov_len'], tar['dec_input'],\n",
        "               q_embed,\n",
        "               c_embed,\n",
        "               tar_embed)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "  #if (epoch + 1) % 2 == 0:\n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "  loss.append(train_loss.result())\n",
        "  accuracy.append(train_accuracy.result())\n",
        "  e.append(epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjGsDYIAhybh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.pyplot.clf()\n",
        "matplotlib.pyplot.plot(e, accuracy)\n",
        "matplotlib.pyplot.xlabel('epochs')\n",
        "matplotlib.pyplot.ylabel('accuracy')\n",
        "matplotlib.pyplot.show()\n",
        "\n",
        "matplotlib.pyplot.plot(e, loss)\n",
        "matplotlib.pyplot.xlabel('epochs')\n",
        "matplotlib.pyplot.ylabel('loss')\n",
        "matplotlib.pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5buvMlnvyrFm",
        "colab": {}
      },
      "source": [
        "def convert_token_ids_to_text(token_ids, batch_size):\n",
        "  output_text = []\n",
        "  for i in range(0,batch_size):\n",
        "    output = tokenizer_all.decode(token_ids[i])\n",
        "    output_text.append([output])\n",
        "  return output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHBLmj1K8qzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(q_embed, c_embed, enc_q_input, enc_c_input, ext_c_input, max_c_oov_len):\n",
        "\n",
        "  output = tf.tile([[tokenizer_all.vocab_size]], [BATCH_SIZE, 1]) # tokenizer_all.vocab_size = start_decoding\n",
        "  output_tokens = tf.tile([['<s>']], [BATCH_SIZE, 1])\n",
        "  #q_embed, c_embed, _ = get_elmo_embeddings(q_tokens, enc_q_len, c_tokens, enc_c_len, tar_tokens, tar_len)  \n",
        "  \n",
        "\n",
        "  for i in range(MAX_A_LEN):\n",
        "    \n",
        "    seq_len = tf.convert_to_tensor(np.repeat(i+1, BATCH_SIZE), tf.int32)#tf.convert_to_tensor(i+1, tf.int32)\n",
        "    pred_tar_embed = elmo_embed_layer({\"tokens\": output_tokens, \"sequence_len\": seq_len})\n",
        "    q_enc_padding_mask, c_enc_padding_mask, combined_mask, q_dec_padding_mask, c_dec_padding_mask = create_masks(enc_q_input, enc_c_input, output)\n",
        "    \n",
        "    predictions, attention_weights = transformer(enc_q_input, enc_c_input, ext_c_input, output, \n",
        "                                False, \n",
        "                                q_enc_padding_mask, \n",
        "                                c_enc_padding_mask,\n",
        "                                combined_mask, \n",
        "                                q_dec_padding_mask,\n",
        "                                c_dec_padding_mask,\n",
        "                                q_embed,\n",
        "                                c_embed,\n",
        "                                pred_tar_embed,\n",
        "                                max_oov_len=max_c_oov_len)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "    output_tokens = tf.concat([output_tokens, tf.convert_to_tensor(convert_token_ids_to_text(predicted_id, BATCH_SIZE), tf.string)], axis=-1)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jqk7Vm9Tl4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import shutil\n",
        " #!gsutil cp -r gs://checkpoints /drive/My Drive/W266/FInalProject/transformer_v2_checkpoints/\n",
        "#shutil.copytree(\"./checkpoint/train3/v11\", \"drive/My Drive/W266/FInalProject/transformer_v7_checkpoints/s_elmo\") \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-hFv0KzYrJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from w266_gutenberg_quiz.library.utility.squad_sparknotes import SquADDataSetSparkNotes\n",
        "\n",
        "data_set = SquADDataSetSparkNotes(data_path=\"w266_gutenberg_quiz/data/sp_squad_document_qa_passages/test.data\")\n",
        "\n",
        "test_size=.1\n",
        "random_state=42\n",
        "question_dev, question_val, context_dev, context_val, answer_dev, answer_val, mc_answers_a_dev, mc_answers_a_val, mc_answers_b_dev, mc_answers_b_val, mc_answers_c_dev, mc_answers_c_val, mc_answers_d_dev, mc_answers_d_val= train_test_split(data_set.questions, \n",
        "                                    data_set.contexts, \n",
        "                                    data_set.answers, \n",
        "                                    data_set.mc_answers_a, \n",
        "                                    data_set.mc_answers_b,\n",
        "                                    data_set.mc_answers_c,\n",
        "                                    data_set.mc_answers_d)\n",
        "\n",
        "filtered_question_dev = []\n",
        "filtered_question_test = []\n",
        "filtered_context_dev = []\n",
        "filtered_context_test = []\n",
        "filtered_answer_dev = []\n",
        "filtered_answer_test = []\n",
        "\n",
        "fil_mc_answers_a_dev=[]\n",
        "fil_mc_answers_b_dev=[] \n",
        "fil_mc_answers_c_dev=[]\n",
        "fil_mc_answers_d_dev=[]\n",
        "\n",
        "\n",
        "## For GPU memory purposes - had to limit the size of the question, passage, and answers\n",
        "max_answer_char_length = 50\n",
        "max_question_char_length = 200\n",
        "max_context_char_length = 2000\n",
        "empty_a = 0\n",
        "for i in range(0, len(question_dev)):\n",
        "  if len(answer_dev[i]) < max_answer_char_length and len(question_dev[i]) < max_question_char_length:\n",
        "    if len(answer_dev[i]) == 0:\n",
        "      #answer_dev[i]='8'\n",
        "      #empty_a = empty_a + 1\n",
        "      continue\n",
        "\n",
        "    if len(context_dev[i]) > max_context_char_length:\n",
        "      context_dev[i] = context_dev[i][:max_context_char_length]\n",
        "    if len(question_dev[i]) > 100:\n",
        "      context_dev[i] = context_dev[i][:100]\n",
        "\n",
        "    filtered_question_dev.append(re.sub(r'([^\\s\\w]|_)+', '', question_dev[i]))\n",
        "    filtered_context_dev.append(re.sub(r'([^\\s\\w]|_)+', '', context_dev[i]))\n",
        "    filtered_answer_dev.append(re.sub(r'([^\\s\\w]|_)+', '', answer_dev[i]))\n",
        "    fil_mc_answers_a_dev.append(mc_answers_a_dev[i])\n",
        "    fil_mc_answers_b_dev.append(mc_answers_b_dev[i])\n",
        "    fil_mc_answers_c_dev.append(mc_answers_c_dev[i])\n",
        "    fil_mc_answers_d_dev.append(mc_answers_d_dev[i])\n",
        "\n",
        "for i in range(0, len(question_val)):\n",
        "  if len(answer_val[i]) < max_answer_char_length and len(question_val[i]) < max_question_char_length and len(context_val[i]) < max_context_char_length:\n",
        "    if len(answer_val[i]) == 0:\n",
        "      answer_val[i]='8'\n",
        "    #  continue\n",
        "    filtered_question_test.append(re.sub(r'([^\\s\\w]|_)+', '',question_val[i]))\n",
        "    filtered_context_test.append(re.sub(r'([^\\s\\w]|_)+', '',context_val[i]))\n",
        "    filtered_answer_test.append(re.sub(r'([^\\s\\w]|_)+', '', answer_val[i]))\n",
        "\n",
        "\n",
        "dev_examples = tf.data.Dataset.from_tensor_slices((filtered_question_dev, filtered_context_dev, filtered_answer_dev, fil_mc_answers_a_dev, fil_mc_answers_b_dev, fil_mc_answers_c_dev, fil_mc_answers_d_dev))\n",
        "dev_dataset = batch_generator(example_generator, np.column_stack((filtered_question_dev, filtered_context_dev, filtered_answer_dev)), training=False)\n",
        "dev_dataset = dev_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmYnKwZOdozn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Full sentence embedding\n",
        "# unlike token by token embedding layer above\n",
        "elmo_pred_embed_layer = hub.KerasLayer('https://tfhub.dev/google/elmo/3', signature=\"default\", trainable=False, output_key=\"default\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYDy4u5dY0DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predicted answers\n",
        "predicted_answers = []\n",
        "predicted_answer_embeddings = []\n",
        "\n",
        "for (batch, (q, c, tar)) in enumerate(dev_dataset):\n",
        "  #print(q['enc_q_input'])\n",
        "  q_embed, c_embed, _ = get_elmo_embeddings(q['question_tokens'], q['enc_q_len'], c['context_tokens'], c['enc_c_len'], tar['answer_tokens'], tar['dec_len']) \n",
        "  results, att = predict(q_embed, c_embed, q['enc_q_input'], c['enc_c_input'], c['extended_enc_c_input'], c['max_c_oov_len'])\n",
        "  for i in range(0, len(results)):\n",
        "\n",
        "    predicted_answer_text = tokenizer_all.decode(results[i])\n",
        "    predicted_answer_text = predicted_answer_text.replace('UNK', '')\n",
        "    predicted_answer_embedding = elmo_pred_embed_layer(tf.expand_dims(predicted_answer_text, axis=0))\n",
        "\n",
        "    predicted_answers.append(predicted_answer_text)\n",
        "    predicted_answer_embeddings.append(predicted_answer_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukvKD5btzS4D",
        "colab_type": "text"
      },
      "source": [
        "### SparkNotes Quiz Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "indjck-6hrYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_answer_choices_by_embedding(predicted_embedding, mc_answers_a_dev, mc_answers_b_dev, mc_answers_c_dev, mc_answers_d_dev, embed_layer):\n",
        "  a_embed = embed_layer(tf.expand_dims(mc_answers_a_dev, axis=0))\n",
        "  b_embed = embed_layer(tf.expand_dims(mc_answers_b_dev, axis=0))\n",
        "  c_embed = embed_layer(tf.expand_dims(mc_answers_c_dev, axis=0))\n",
        "  d_embed = embed_layer(tf.expand_dims(mc_answers_d_dev, axis=0))\n",
        "\n",
        "  a_sim = cosine_similarity(predicted_embedding, a_embed)\n",
        "  b_sim = cosine_similarity(predicted_embedding, b_embed)\n",
        "  c_sim = cosine_similarity(predicted_embedding, c_embed)\n",
        "  d_sim = cosine_similarity(predicted_embedding, d_embed)\n",
        "\n",
        "  return [a_sim, b_sim, c_sim, d_sim]\n",
        "\n",
        "def get_answer_choices_by_bleu(predicted_answer, mc_answers_a_dev, mc_answers_b_dev, mc_answers_c_dev, mc_answers_d_dev):\n",
        "  bleu1_scores = []\n",
        "  bleu1_scores.append(nltk.translate.bleu_score.sentence_bleu([mc_answers_a_dev.numpy().decode()], predicted_answer, weights=([1])))\n",
        "  bleu1_scores.append(nltk.translate.bleu_score.sentence_bleu([mc_answers_b_dev.numpy().decode()], predicted_answer, weights=([1])))\n",
        "  bleu1_scores.append(nltk.translate.bleu_score.sentence_bleu([mc_answers_c_dev.numpy().decode()], predicted_answer, weights=([1])))\n",
        "  bleu1_scores.append(nltk.translate.bleu_score.sentence_bleu([mc_answers_d_dev.numpy().decode()], predicted_answer, weights=([1])))\n",
        "\n",
        "  return bleu1_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpsaHFake1TX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate_sparknotes_quiz(predicted_answers, predicted_answer_embeddings, sp_quiz_questions, elmo_pred_embed_layer):\n",
        "  i=0\n",
        "  num_embed_correct = 0\n",
        "  num_bleu_correct = 0\n",
        "  not_zero = 0\n",
        "\n",
        "  bleu1_scores = []\n",
        "  bleu4_scores = []\n",
        "\n",
        "\n",
        "  for question_dev, context_dev, answer_dev, mc_answers_a_dev, mc_answers_b_dev, mc_answers_c_dev, mc_answers_d_dev in sp_quiz_questions:\n",
        "    if i >= len(predicted_answer_embeddings):\n",
        "      break\n",
        "\n",
        "    correct_index = 0\n",
        "    if mc_answers_b_dev == answer_dev:\n",
        "      correct_index = 1\n",
        "    elif mc_answers_b_dev == answer_dev:\n",
        "      correct_index = 2\n",
        "    elif mc_answers_b_dev == answer_dev:\n",
        "      correct_index = 3\n",
        "\n",
        "    embed_sims = get_answer_choices_by_embedding(predicted_answer_embeddings[i],  mc_answers_a_dev, mc_answers_b_dev, mc_answers_c_dev, mc_answers_d_dev, elmo_pred_embed_layer)\n",
        "    bleu1_sims = get_answer_choices_by_bleu(predicted_answers[i], mc_answers_a_dev, mc_answers_b_dev, mc_answers_c_dev, mc_answers_d_dev)\n",
        "    embed_match_index = np.argmax(embed_sims)\n",
        "    bleu1_match_index = np.argmax(bleu1_sims)\n",
        "    \n",
        "    bleu1_scores.append(nltk.translate.bleu_score.sentence_bleu([answer_dev.numpy().decode()], predicted_answers[i], weights=([1])))\n",
        "    bleu4_scores.append(nltk.translate.bleu_score.sentence_bleu([answer_dev.numpy().decode()], predicted_answers[i], weights=(.25, .25, .25, .25)))\n",
        "\n",
        "    if embed_match_index == correct_index:\n",
        "      num_embed_correct = num_embed_correct + 1\n",
        "    if bleu1_match_index == correct_index:\n",
        "      num_bleu_correct = num_bleu_correct + 1\n",
        "\n",
        "    i = i + 1\n",
        "\n",
        "  quiz_embed_accuracy = num_embed_correct / i\n",
        "  quiz_bleu1_accuracy = num_bleu_correct / i\n",
        "\n",
        "  return quiz_embed_accuracy, quiz_bleu1_accuracy, bleu1_scores, bleu4_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z5yev_B2bnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quiz_embed_accuracy, quiz_bleu1_accuracy, bleu1_scores, bleu4_scores = evaluate_sparknotes_quiz(predicted_answers, predicted_answer_embeddings, dev_examples, elmo_pred_embed_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss_aPZYCDm7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'quiz accuracy: {quiz_embed_accuracy}')\n",
        "print(f'quiz bleu1 accuracy: {quiz_bleu1_accuracy}')\n",
        "print(f'bleu1: {np.mean(bleu1_scores)}')\n",
        "print(f'bleu4: {np.mean(bleu4_scores)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErVQwcMe2ceq",
        "colab_type": "text"
      },
      "source": [
        "##BiDAF - Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmvbwOnF2f0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors import Predictor\n",
        "predictor = Predictor.from_path(\"https://allennlp.s3.amazonaws.com/models/bidaf-model-2017.09.15-charpad.tar.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIBm0iJM3LNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_answers = []\n",
        "predicted_answer_embeddings = []\n",
        "\n",
        "for filtered_question_dev, filtered_context_dev, filtered_answer_dev, fil_mc_answers_a_dev, fil_mc_answers_b_dev, fil_mc_answers_c_dev, fil_mc_answers_d_dev in dev_examples:\n",
        "\n",
        "  results = predictor.predict(filtered_question_dev.numpy().decode(), passage=filtered_context_dev.numpy().decode())\n",
        "  predicted_answer_text = results['best_span_str']\n",
        "  predicted_answer_embedding = elmo_pred_embed_layer(tf.expand_dims(predicted_answer_text, axis=0))\n",
        "\n",
        "  predicted_answers.append(predicted_answer_text)\n",
        "  predicted_answer_embeddings.append(predicted_answer_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}